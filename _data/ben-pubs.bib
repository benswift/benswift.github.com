
@inproceedings{swiftChasingWindExperience2011,
  title = {A {{Chasing After}} the {{Wind}}: {{Experience}} in {{Computer}}-Supported {{Group Musicmaking}}},
  url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.460.5123},
  eventtitle = {British {{HCI Group Annual Conference}} on {{People}} and {{Computers}}},
  booktitle = {In {{Proceedings}} of "{{When}} Words Fail: {{What}} Can Music Interaction Tell Us about {{HCI}}?" Workshop},
  date = {2011-05-08},
  author = {Swift, Ben and Gardner, Henry and Riddell, Alistair},
  file = {/Users/ben/Documents/Zotero/storage/URQ7F4R2/swift_et_al_2011_a_chasing_after_the_wind.pdf}
}

@article{swiftImpishGrooves2011,
  title = {Impish {{Grooves}}},
  volume = {35},
  url = {http://www.mitpressjournals.org/doi/abs/10.1162/COMJ_e_00098},
  number = {4},
  journaltitle = {Computer Music Journal},
  shortjournal = {Computer Music Journal},
  date = {2011-01-01},
  pages = {119-137},
  author = {Swift, Ben}
}

@inproceedings{swiftDistributedPerformanceLive2009,
  title = {Distributed {{Performance}} in {{Live Coding}}},
  url = {https://computermusic.org.au/conferences/acmc2009/},
  eventtitle = {{{ACMC}} '09: {{Proceedings}} of the {{Australasian Computer Music Conference}}},
  booktitle = {Proceedings of the 2009 {{Annual Conference}} of the {{Australasian Computer Music Association}}},
  date = {2009-07},
  pages = {1--6},
  author = {Swift, Ben and Gardner, Henry J and Riddell, Alistair},
  file = {/Users/ben/Documents/Zotero/storage/3KI47PYM/swift_et_al_2009_distributed_performance_in_live_coding.pdf}
}

@incollection{swiftChasingFeelingExperience2013,
  location = {{London}},
  title = {Chasing a {{Feeling}}: {{Experience}} in {{Computer Supported Jamming}}},
  isbn = {978-1-4471-2990-5},
  url = {https://doi.org/10.1007/978-1-4471-2990-5_5},
  doi = {10.1007/978-1-4471-2990-5_5},
  abstract = {Improvisational group music-making, informally known as ‘jamming’, has its own cultures and conventions of musical interaction. One characteristic of this interaction is the primacy of the experience over the musical artefact—in some sense the sound created is not as important as the feeling of being ‘in the groove’. As computing devices infiltrate creative, open-ended task domains, what can Human-Computer Interaction (HCI) learn from jamming? How do we design systems where the goal is not an artefact but a felt experience? This chapter examines these issues in light of an experiment involving ‘Viscotheque’, a novel group music-making environment based on the iPhone.},
  booktitle = {Music and {{Human}}-{{Computer Interaction}}},
  publisher = {{Springer London}},
  date = {2013},
  pages = {85-99},
  author = {Swift, Ben},
  editor = {Holland, Simon and Wilkie, Katie and Mulholland, Paul and Seago, Allan},
  file = {/Users/ben/Documents/Zotero/storage/8ARS87WI/swift_2013_chasing_a_feeling.pdf}
}

@inproceedings{swiftVisualCodeAnnotations2013,
  title = {Visual Code Annotations for Cyberphysical Programming},
  url = {https://ieeexplore.ieee.org/document/6617345},
  doi = {10.1109/LIVE.2013.6617345},
  abstract = {User interfaces for source code editing are a crucial component in any software development environment, and in many editors visual annotations (overlaid on the textual source code) are used to provide important contextual information to the programmer. This paper focuses on the real-time programming activity of `cyberphysical' programming, and considers the type of visual annotations which may be helpful in this programming context.},
  eventtitle = {2013 1st {{International Workshop}} on {{Live Programming}} ({{LIVE}})},
  booktitle = {2013 1st {{International Workshop}} on {{Live Programming}} ({{LIVE}})},
  date = {2013-05},
  pages = {27-30},
  keywords = {Context,contextual information,cyberphysical programming,interactive programming,Production,program compilers,Programming,programming context,real-time programming activity,Robots,Software,software development environment,source code editing,System-on-chip,textual source code,user interfaces,visual code annotations,visual programming,Visualization},
  author = {Swift, Ben and Sorensen, Andrew and Gardner, Henry J. and Hosking, John},
  file = {/Users/ben/Documents/Zotero/storage/QNMKPMGZ/swift_et_al_2013_visual_code_annotations_for_cyberphysical_programming.pdf}
}

@inproceedings{swiftEngagementNetworksSocial2010,
  location = {{New York, NY, USA}},
  title = {Engagement {{Networks}} in {{Social Music}}-Making},
  isbn = {978-1-4503-0502-0},
  url = {http://doi.acm.org/10.1145/1952222.1952244},
  doi = {10.1145/1952222.1952244},
  abstract = {Social music-making systems offer the possibility of accessible and engaging group experiences. In this paper we explore questions concerning the notion of 'engagement' in social music-making. In a recent user study of Viscotheque, an iPhone-based environment for group musical creativity, three different types of engagement were observed: individual, unilateral and bilateral. These results indicate that network-based approaches may be useful in analysing engagement relationships amongst participants in group music-making.},
  booktitle = {Proceedings of the {{22Nd Conference}} of the {{Computer}}-{{Human Interaction Special Interest Group}} of {{Australia}} on {{Computer}}-{{Human Interaction}}},
  series = {{{OZCHI}} '10},
  publisher = {{ACM}},
  urldate = {2019-01-23},
  date = {2010},
  pages = {104--111},
  keywords = {engagement,mutual},
  author = {Swift, Ben and Gardner, Henry and Riddell, Alistair},
  file = {/Users/ben/Documents/Zotero/storage/K7CQUSEK/swift_et_al_2010_engagement_networks_in_social_music-making.pdf}
}

@inproceedings{martinExploringPercussiveGesture2014,
  location = {{New York, NY, USA}},
  title = {Exploring {{Percussive Gesture}} on {{iPads}} with {{Ensemble Metatone}}},
  isbn = {978-1-4503-2473-1},
  url = {http://doi.acm.org/10.1145/2556288.2557226},
  doi = {10.1145/2556288.2557226},
  abstract = {Percussionists are unique among western classical instrumentalists in that their artistic practice is defined by an approach to interaction rather than their instruments. While percussionists are accustomed to exploring non-traditional objects to create music, these objects have yet to encompass touch-screen computing devices to any great extent. The proliferation and popularity of these devices now presents an opportunity to explore their use in combining computer-generated sound together with percussive interaction in a musical ensemble. This paper examines Ensemble Metatone, a group formed to explore the "infiltration" of iPad-based musical instruments into a free-improvisation percussion ensemble. We discuss the design approach for two different iPad percussion instruments and the methodology for exploring them with the group over a series of rehearsals and performances. Qualitative analysis of discussions throughout this process shows that the musicians developed a vocabulary of gestures and musical interactions to make musical sense of these new instruments.},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  series = {{{CHI}} '14},
  publisher = {{ACM}},
  urldate = {2019-01-23},
  date = {2014},
  pages = {1025--1028},
  keywords = {multitouch,user experience,music,gesture,expression,percussion},
  author = {Martin, Charles and Gardner, Henry and Swift, Ben},
  file = {/Users/ben/Documents/Zotero/storage/GQWAWBLD/martin_et_al_2014_exploring_percussive_gesture_on_ipads_with_ensemble_metatone.pdf}
}

@article{sorensenManyMeaningsLive2014,
  title = {The {{Many Meanings}} of {{Live Coding}}},
  volume = {38},
  issn = {0148-9267},
  url = {https://doi.org/10.1162/COMJ_a_00230},
  doi = {10.1162/COMJ_a_00230},
  abstract = {The ten-year anniversary of TOPLAP presents a unique opportunity for reflection and introspection. In this essay we ask the question, what is the meaning of live coding? Our goal is not to answer this question, in absolute terms, but rather to attempt to unpack some of live coding's many meanings. Our hope is that by exploring some of the formal, embodied, and cultural meanings surrounding live-coding practice, we may help to stimulate a conversation that will resonate within the live-coding community for the next ten years.},
  number = {1},
  journaltitle = {Computer Music Journal},
  shortjournal = {Computer Music Journal},
  urldate = {2019-01-23},
  date = {2014-03-01},
  pages = {65-76},
  author = {Sorensen, Andrew and Swift, Ben and Riddell, Alistair},
  file = {/Users/ben/Documents/Zotero/storage/2G22IU54/sorensen_et_al_2014_the_many_meanings_of_live_coding.pdf}
}

@inproceedings{swiftCodingLivecoding2014,
  location = {{New York, NY, USA}},
  title = {Coding {{Livecoding}}},
  isbn = {978-1-4503-2473-1},
  url = {http://doi.acm.org/10.1145/2556288.2557049},
  doi = {10.1145/2556288.2557049},
  abstract = {Livecoding is an artistic programming practice in which an artist's low-level interaction can be observed with sufficiently high fidelity to allow for transcription and analysis. This paper presents the first reported "coding" of livecoding videos. From an identified corpus of videos available on the web, we coded performances of two different livecoding artists, recording both the (textual) programming edit events and the musical effect of these edits. Our analysis includes a novel, transition-matrix visualisation of the textual and musical dimensions of this data to create a "performer fingerprint". We show how detailed transcriptions of livecoding videos can be made which, we hope, will provide a foundation for further research into describing and understanding livecoding.},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  series = {{{CHI}} '14},
  publisher = {{ACM}},
  urldate = {2019-01-23},
  date = {2014},
  pages = {1021--1024},
  keywords = {end-user programming,creativity support tools},
  author = {Swift, Ben and Sorensen, Andrew and Martin, Michael and Gardner, Henry},
  file = {/Users/ben/Documents/Zotero/storage/8XGUHV48/swift_et_al_2014_coding_livecoding.pdf}
}

@article{wuTextureAnalysis3D2015,
  title = {Texture Analysis of the {{3D}} Collagen Network and Automatic Classification of the Physiology of Articular Cartilage {{AU}}  - {{Duan}}, {{Xiaojuan}}},
  volume = {18},
  issn = {1025-5842},
  url = {https://doi.org/10.1080/10255842.2013.864284},
  doi = {10.1080/10255842.2013.864284},
  number = {9},
  journaltitle = {Computer Methods in Biomechanics and Biomedical Engineering},
  shortjournal = {Computer Methods in Biomechanics and Biomedical Engineering},
  date = {2015-07-04},
  pages = {931-943},
  author = {Wu, Jianping and Swift, Ben and Kirk, Thomas Brett},
  file = {/Users/ben/Documents/Zotero/storage/2D2IABJ8/wu_et_al_2015_texture_analysis_of_the_3d_collagen_network_and_automatic_classification_of_the.pdf}
}

@inproceedings{martinIntelligentAgentsNetworked2016,
  location = {{New York, NY, USA}},
  title = {Intelligent {{Agents}} and {{Networked Buttons Improve Free}}-{{Improvised Ensemble Music}}-{{Making}} on {{Touch}}-{{Screens}}},
  isbn = {978-1-4503-3362-7},
  url = {http://doi.acm.org/10.1145/2858036.2858269},
  doi = {10.1145/2858036.2858269},
  abstract = {We present the results of two controlled studies of free-improvised ensemble music-making on touch-screens. In our system, updates to an interface of harmonically-selected pitches are broadcast to every touch-screen in response to either a performer pressing a GUI button, or to interventions from an intelligent agent. In our first study, analysis of survey results and performance data indicated significant effects of the button on performer preference, but of the agent on performance length. In the second follow-up study, a mixed-initiative interface, where the presence of the button was interlaced with agent interventions, was developed to leverage both approaches. Comparison of this mixed-initiative interface with the always-on button-plus-agent condition of the first study demonstrated significant preferences for the former. The different approaches were found to shape the creative interactions that take place. Overall, this research offers evidence that an intelligent agent and a networked GUI both improve aspects of improvised ensemble music-making.},
  booktitle = {Proceedings of the 2016 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  series = {{{CHI}} '16},
  publisher = {{ACM}},
  urldate = {2019-01-23},
  date = {2016},
  pages = {2295--2306},
  keywords = {design,creativity support tools,agent,collaborative interaction,mobile music},
  author = {Martin, Charles and Gardner, Henry and Swift, Ben and Martin, Michael},
  file = {/Users/ben/Documents/Zotero/storage/QFRX2GJP/martin_et_al_2016_intelligent_agents_and_networked_buttons_improve_free-improvised_ensemble.pdf}
}

@inproceedings{martinMetatravelsMetalonsdaleIpad2014,
  location = {{New York, NY, USA}},
  title = {Metatravels and {{Metalonsdale}}: {{Ipad Apps}} for {{Percussive Improvisation}}},
  isbn = {978-1-4503-2474-8},
  url = {http://doi.acm.org/10.1145/2559206.2574805},
  doi = {10.1145/2559206.2574805},
  shorttitle = {Metatravels and {{Metalonsdale}}},
  abstract = {Percussionists are unique among instrumentalists in that their artistic practice is defined by an approach to interaction rather than their instruments. While percussionists are accustomed to exploring non-traditional objects to create music, these objects have yet to encompass touch-screen computing devices to any great extent. The proliferation and popularity of these devices now presents an opportunity to explore their use in combining computer-generated sound together with percussive interaction in a musical ensemble. This interactivity demonstration presents two iPad-instruments developed in collaboration with Ensemble Metatone, a group formed to explore the "infiltration" of iPad apps into a free-improvisation percussion ensemble. The apps encourage the performers' exploration through percussive gestures and use network features to support cohesive improvisation.},
  booktitle = {{{CHI}} '14 {{Extended Abstracts}} on {{Human Factors}} in {{Computing Systems}}},
  series = {{{CHI EA}} '14},
  publisher = {{ACM}},
  urldate = {2019-01-23},
  date = {2014},
  pages = {547--550},
  keywords = {multitouch,user experience,music,gesture,artistic research,expression,percussion},
  author = {Martin, Charles and Gardner, Henry and Swift, Ben},
  file = {/Users/ben/Documents/Zotero/storage/7RVXPLGR/martin_et_al_2014_metatravels_and_metalonsdale.pdf}
}

@inproceedings{swiftBecomingsoundAffectAssemblage2012,
  location = {{New York, NY, USA}},
  title = {Becoming-Sound: {{Affect}} and {{Assemblage}} in {{Improvisational Digital Music Making}}},
  isbn = {978-1-4503-1015-4},
  url = {http://doi.acm.org/10.1145/2207676.2208315},
  doi = {10.1145/2207676.2208315},
  shorttitle = {Becoming-Sound},
  abstract = {The concepts of affect and assemblage proposed by thinkers such as Gilles Deleuze and Brian Massumi can help us to understand the interaction between users and artefacts in interactive systems, particularly in the context of computer-supported improvisation and creativity. In this paper I provide an introduction to affect and assemblage theory for HCI practitioners. I then use a case study of Viscotheque, an iOS-based interface for group musical collaboration, to demonstrate the application of affective analysis in making sense of improvisational group music making.},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  series = {{{CHI}} '12},
  publisher = {{ACM}},
  urldate = {2019-01-23},
  date = {2012},
  pages = {1815--1824},
  keywords = {affect,assemblage,improvisation,music making},
  author = {Swift, Ben},
  file = {/Users/ben/Documents/Zotero/storage/GJ87V5RI/swift_2012_becoming-sound.pdf}
}

@inproceedings{swiftMindmodulatedMusicMind2007,
  location = {{New York, NY, USA}},
  title = {Mind-Modulated {{Music}} in the {{Mind Attention Interface}}},
  isbn = {978-1-59593-872-5},
  url = {http://doi.acm.org/10.1145/1324892.1324907},
  doi = {10.1145/1324892.1324907},
  abstract = {A recent study of electroencephalogram (EEG) activity associated with musical cognition has suggested a correlate for the amount of active musical processing taking place in the brains of musicians. Using a version of this measure, we have built a new brain computer interface which harnesses the "natural" brain activity of musicians to mold and modulate music as it is being composed and played. This computer music instrument is part of a system, the Mind Attention Interface, which provides an interface to a virtual reality theatre using measures of a participant's EEG, eye-gaze and head position. The theatre itself, and its spatialised sound system, closes a feedback loop through the mind of the participant.},
  booktitle = {Proceedings of the 19th {{Australasian Conference}} on {{Computer}}-{{Human Interaction}}: {{Entertaining User Interfaces}}},
  series = {{{OZCHI}} '07},
  publisher = {{ACM}},
  urldate = {2019-01-23},
  date = {2007},
  pages = {83--86},
  keywords = {computer music,brain computer interface,EEG,functional connectivity},
  author = {Swift, Ben and Sheridan, James and Zhen, Yang and Gardner, Henry J.},
  file = {/Users/ben/Documents/Zotero/storage/P7NEGBWQ/swift_et_al_2007_mind-modulated_music_in_the_mind_attention_interface.pdf}
}

@article{wuHighresolutionStudy3D2017,
  langid = {french},
  title = {High-resolution study of the 3D collagen fibrillary matrix of Achilles tendons without tissue labelling and dehydrating},
  volume = {266},
  issn = {1365-2818},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jmi.12537},
  doi = {10.1111/jmi.12537},
  abstract = {Knowledge of the collagen structure of an Achilles tendon is critical to comprehend the physiology, biomechanics, homeostasis and remodelling of the tissue. Despite intensive studies, there are still uncertainties regarding the microstructure. The majority of studies have examined the longitudinally arranged collagen fibrils as they are primarily attributed to the principal tensile strength of the tendon. Few studies have considered the structural integrity of the entire three-dimensional (3D) collagen meshwork, and how the longitudinal collagen fibrils are integrated as a strong unit in a 3D domain to provide the tendons with the essential tensile properties. Using second harmonic generation imaging, a 3D imaging technique was developed and used to study the 3D collagen matrix in the midportion of Achilles tendons without tissue labelling and dehydration. Therefore, the 3D collagen structure is presented in a condition closely representative of the in vivo status. Atomic force microscopy studies have confirmed that second harmonic generation reveals the internal collagen matrix of tendons in 3D at a fibril level. Achilles tendons primarily contain longitudinal collagen fibrils that braid spatially into a dense rope-like collagen meshwork and are encapsulated or wound tightly by the oblique collagen fibrils emanating from the epitenon region. The arrangement of the collagen fibrils provides the longitudinal fibrils with essential structural integrity and endows the tendon with the unique mechanical function for withstanding tensile stresses. A novel 3D microscopic method has been developed to examine the 3D collagen microstructure of tendons without tissue dehydrating and labelling. The study also provides new knowledge about the collagen microstructure in an Achilles tendon, which enables understanding of the function of the tissue. The knowledge may be important for applying surgical and tissue engineering techniques to tendon reconstruction.},
  number = {3},
  journaltitle = {Journal of Microscopy},
  urldate = {2019-01-23},
  date = {2017},
  pages = {273-287},
  keywords = {3D collagen structure,Achilles tendons,computer imaging analysis,SHG imaging},
  author = {Wu, Jian-Ping and Swift, Ben and Becker, Thomas and Squelch, Andrew and Wang, Allan and Zheng, Yong-Chang and Zhao, Xuelin and Xu, Jiake and Xue, Wei and Zheng, Minghao and Lloyd, David and Kirk, Thomas Brett},
  file = {/Users/ben/Documents/Zotero/storage/GHQSAZG2/wu_et_al_2017_high-resolution_study_of_the_3d_collagen_fibrillary_matrix_of_achilles_tendons.pdf}
}

@article{chrzeszczykInfiniCloudDistributingHigh2016,
  langid = {english},
  title = {{{InfiniCloud}} 2.0: Distributing {{High Performance Computing}} across Continents.},
  volume = {3},
  issn = {2313-8734},
  url = {http://superfri.org/superfri/article/view/95},
  doi = {10.14529/jsfi160204},
  shorttitle = {{{InfiniCloud}} 2.0},
  abstract = {InfiniCloud 2.0 is World’s first native InfiniBand High Performance Cloud distributed across four continents, spanning Asia, Australia, Europe and North America. The project provides researchers with instant access to computational, storage and network resources distributed around the globe. These resources are then used to build a geographically distributed, virtual supercomputer, complete with globally-accessible parallel file system and job scheduling.This paper describes high level design and the implementation details of InfiniCloud 2.0. A gene sequencing pipeline as well as plasma physics simulation code are used to demonstrate system’s capabilities.},
  number = {2},
  journaltitle = {Supercomputing Frontiers and Innovations},
  urldate = {2019-01-23},
  date = {2016-06-27},
  pages = {54-71-71},
  author = {Chrzeszczyk, Jakub and Howard, Andrew and Chrzeszczyk, Andrzej and Swift, Ben and Davis, Peter and Low, Jonathan and Tan, Tin Wee and Ban, Kenneth},
  file = {/Users/ben/Documents/Zotero/storage/Q6PTUNLN/chrzeszczyk_et_al_2016_infinicloud_2.pdf}
}

@article{swiftLiveProgrammingScientific2016,
  langid = {english},
  title = {Live {{Programming}} in {{Scientific Simulation}}},
  volume = {2},
  issn = {2313-8734},
  url = {http://superfri.org/superfri/article/view/72},
  doi = {10.14529/jsfi150401},
  abstract = {We demonstrate that a live-programming environment can be used to harness and add run-time interactivity to scientific simulation codes. Through a set of examples using a Particle-In-Cell (PIC) simulation framework we show how the real-time, human-in-the-loop interactivity of live programming can be incorporated into a traditional, “offline”, development workflow. We discuss how live programming tools and techniques can be productively integrated into the existing HPC landscape to increase productivity and enhance exploration and discovery.},
  number = {4},
  journaltitle = {Supercomputing Frontiers and Innovations},
  urldate = {2019-01-23},
  date = {2016-03-07},
  pages = {4-15-15},
  author = {Swift, Ben and Sorensen, Andrew and Gardner, Henry and Davis, Peter and Decyk, Viktor K.},
  file = {/Users/ben/Documents/Zotero/storage/IGLQLS6W/swift_et_al_2016_live_programming_in_scientific_simulation.pdf}
}

@incollection{browneCriticalChallengesVisual2018,
  langid = {english},
  location = {{Cham}},
  title = {Critical {{Challenges}} for the {{Visual Representation}} of {{Deep Neural Networks}}},
  isbn = {978-3-319-90403-0},
  url = {https://doi.org/10.1007/978-3-319-90403-0_7},
  doi = {10.1007/978-3-319-90403-0_7},
  abstract = {Artificial neural networks have proved successful in a broad range of applications over the last decade. However, there remain significant concerns about their interpretability. Visual representation is one way researchers are attempting to make sense of these models and their behaviour. The representation of neural networks raises questions which cross disciplinary boundaries. This chapter draws on a growing collection of interdisciplinary scholarship regarding neural networks. We present six case studies in the visual representation of neural networks and examine the particular representational challenges posed by these algorithms. Finally we summarise the ideas raised in the case studies as a set of takeaways for researchers engaging in this area.},
  booktitle = {Human and {{Machine Learning}}: {{Visible}}, {{Explainable}}, {{Trustworthy}} and {{Transparent}}},
  series = {Human–{{Computer Interaction Series}}},
  publisher = {{Springer International Publishing}},
  urldate = {2019-01-23},
  date = {2018},
  pages = {119-136},
  author = {Browne, Kieran and Swift, Ben and Gardner, Henry},
  editor = {Zhou, Jianlong and Chen, Fang},
  file = {/Users/ben/Documents/Zotero/storage/LTQGGT7Y/browne_et_al_2018_critical_challenges_for_the_visual_representation_of_deep_neural_networks.pdf}
}

@inproceedings{browneOtherSideAlgorithm2018,
  location = {{New York, NY, USA}},
  title = {The {{Other Side}}: {{Algorithm As Ritual}} in {{Artificial Intelligence}}},
  isbn = {978-1-4503-5621-3},
  url = {http://doi.acm.org/10.1145/3170427.3188404},
  doi = {10.1145/3170427.3188404},
  shorttitle = {The {{Other Side}}},
  abstract = {Our cultural and scientific understandings of neural networks are built on a set of philosophical ideas which might turn out to be superstitions. Drawing on methodologies of defamiliarisation and performance art which have been adopted by HCI, we present an analog apparatus for the ritualistic performance of neural network algorithms. The apparatus draws on the interaction modes of the Ouija board to provide a system which involves the user in the computation. By recontextualising neural computation, the work creates critical distance with which to examine the philosophical and cultural assumptions embedded in our conception of AI.},
  booktitle = {Extended {{Abstracts}} of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  series = {{{CHI EA}} '18},
  publisher = {{ACM}},
  urldate = {2019-01-23},
  date = {2018},
  pages = {alt11:1--alt11:9},
  keywords = {defamiliarisation,neural networks,performance art,ritual,séance},
  author = {Browne, Kieran and Swift, Ben},
  file = {/Users/ben/Documents/Zotero/storage/ZJLBQ7DP/browne_swift_2018_the_other_side.pdf}
}

@inproceedings{purcellVisualisingLiveCoding2014,
  location = {{New York, NY, USA}},
  title = {Visualising a {{Live Coding Arts Process}}},
  isbn = {978-1-4503-0653-9},
  url = {http://doi.acm.org/10.1145/2686612.2686634},
  doi = {10.1145/2686612.2686634},
  abstract = {This paper describes an empirical study of source code visualisation as a means to communicate the programming process in "live coding" computer music performances. Following an exploratory field study of a live-coding performance at an arts festival, two different interaction-driven visualisation techniques were incorporated into a live coding system. We then performed a more controlled laboratory study to evaluate the visualisations' contributions to the audience experience, with emphasis on the (self-reported) experiential dimensions of understanding and enjoyment. Both software visualisation techniques enhanced audience enjoyment, while the effect on audience understanding was more complex. We conclude by suggesting how these visualisation techniques may be used to enhance the audience experience of live coding.},
  booktitle = {Proceedings of the 26th {{Australian Computer}}-{{Human Interaction Conference}} on {{Designing Futures}}: {{The Future}} of {{Design}}},
  series = {{{OzCHI}} '14},
  publisher = {{ACM}},
  urldate = {2019-01-23},
  date = {2014},
  pages = {141--144},
  keywords = {live coding,musical performance,software visualisation},
  author = {Purcell, Arrian and Gardner, Henry and Swift, Ben},
  file = {/Users/ben/Documents/Zotero/storage/Q2HB99PG/purcell_et_al_2014_visualising_a_live_coding_arts_process.pdf}
}

@inproceedings{martinMusic18Performances2015,
  title = {Music of 18 Performances: {{Evaluating}} Apps and Agents with Free Improvisation},
  isbn = {1448-7780},
  url = {https://computermusic.org.au/conferences/acmc-2015/},
  booktitle = {Proceedings of the 2015 {{Annual Conference}} of the {{Australasian Computer Music Association}}},
  publisher = {{The Australasian Computer Music Association}},
  date = {2015},
  author = {Martin, Charles and Gardner, Henry and Swift, Ben and Martin, Michael},
  file = {/Users/ben/Documents/Zotero/storage/S8N62FYD/martin_et_al_2015_music_of_18_performances.pdf}
}

@inproceedings{swiftNetworkedLivecodingVL2014,
  title = {Networked Livecoding at {{VL}}/{{HCC}} 2013},
  isbn = {978-1-4799-4035-6},
  url = {https://doi.ieeecomputersociety.org/10.1109/VLHCC.2014.6883065},
  doi = {10.1109/VLHCC.2014.6883065},
  abstract = {Network connectivity offers the potential for a group of musicians to play together over the network. This paper describes a trans-Atlantic networked musical livecoding performance between Andrew Sorensen in Germany (at the Schloss Daghstuhl conference on Collaboration and Learning through Live Coding) and Ben Swift in San Jose (at YL/HCC) in September 2013. In this paper we describe the infrastructure developed to enable this performance.},
  booktitle = {2014 {{IEEE Symposium}} on {{Visual Languages}} and {{Human}}-{{Centric Computing}} ({{VL}}/{{HCC}})({{VLHCC}})},
  urldate = {2019-01-23},
  date = {2014-07},
  pages = {221-222},
  keywords = {Australia,Bandwidth,Collaboration,Computer architecture,Educational institutions,Encoding,Servers},
  author = {Swift, Ben and Gardner, H. and Sorensen, A.},
  file = {/Users/ben/Documents/Zotero/storage/ZQYYUYDI/06883065-abs.html}
}

@inproceedings{wangAnalysisVisualizationNarrative2019,
  title = {Analysis and {{Visualization}} of {{Narrative}} in {{Shanhaijing Using Linked Data}}},
  url = {https://dh2019.adho.org},
  abstract = {This paper seeks to bridge that gap by creating, analyzing and publishing a case study example - the Chinese classic 'Shanhaijing' (Classic of Mountains and Seas, 山海经) - using Linked Data methods. We recount the complexities of representing ancient Chinese literary narratives, captured through a close reading of the narrative (unstructured) data in both English and Classical Chinese. We evaluate the challenges in using tools developed from Western perspectives and for complete and largely homogeneous, highly-structured data for the capture of the characteristics and related information about the monsters and mythical creatures described within the 'Shanhaijing'. We present a purpose-built user-interface, which allows users to explore this data both with and without needing to write SPARQL queries.},
  booktitle = {{{DH}} 2019: {{Proceedings}} of the {{International}} Conference of the {{Alliance}} of {{Digital Humanities Organizations}}},
  date = {2019},
  author = {Wang, Qian and Nurmikko-Fuller, Terhi and Swift, Ben},
  file = {/Users/ben/Documents/Zotero/storage/FG3E26RF/wang_et_al_2019_analysis_and_visualization_of_narrative_in_shanhaijing_using_linked_data.pdf}
}

@inproceedings{swiftThoughtsANULaptop2018,
  location = {{Swinburne University, Melbourne, Australia}},
  title = {Thoughts on an {{ANU Laptop Ensemble}}: Where Do New Media Creators Fit in the Modern University?},
  url = {http://www.code2k18.com},
  abstract = {Computer Science student numbers are exploding around the country. Economic realities and future employment prospects are certainly part of this trend, as well as parental \& societal pressure and many other factors. However, not all of these CS students want to go to Silicon Valley post-graduation and earn a billion dollars – some want to be cultural creators, indeed many already are (arts, music, memes, etc.). The core CS curriculum is still fairly traditional, and "creative code" activities usually fall into elective courses at best (or, indeed, are forced into spare time as hobbies).
 
In addition, CS has well-documented diversity problems. This is a wicked problem which requires addressing at many levels, but most would agree that part of the solution is having ways to attract these new media creators (e.g. STEAM rather than just STEM).
 
On the other side of campus, prevailing forces over the last couple of decades have not been kind to the creative departments (Schools of Music, Schools of Art, and the humanities in general-–-with a few notable exceptions). University funding is governed by economic realities, and these art \& craft-based disciplines, which by their very nature require high student to teacher ratios, are finding themselves increasingly difficult to justify in that context.
 
There is a lot of rhetoric about the importance of cross-campus and interdisciplinary collaboration, including in the creative arts/technology space. However, most of the energy and excitement surrounding cross-campus collaboration seems to be in "innovation/entrepreneurship" programs. While these programs often do lead to positive outcomes, there is usually a strong commercial imperative in an innovation context, so it's not always easy to see where innovative creative applications of technology fit into this world.
 
Our provocation is that the modern university has a huge untapped potential for new media content creation, but departmental silos stifle its flourishing, and that current 'de-silofication' initiatives (e.g. innovation/entrepreneurship) are too crassly-commercial to support many forms of collaborative content creation. The question is: what pragmatic, short-lead-time activities can we do support to release this untapped potential?
 
Over the last few years, we, two young academics-–-one, a live-coder and lecturer in Computer Science, the other, a composer/improviser from Music-–-have been finding ways to bring our students together in meaningful collaborative multimedia projects. Recently we have formalised these previously extracurricular activities in the new ANU Laptop Ensemble.
 
This group of \textasciitilde{}10 students, from Computer Science, Art and Music, are currently working together as a pilot program using an existing "group project course" infrastructure primarily designed for industry client-based tech entrepreneurship projects for later-year CS students. In this we assume the roles (client and tutor) usually filled by industry mentors, helping students find their way through the seemingly infinite possibilities of digital performance that lie waiting inside every laptop and mobile device. The client's brief: to provide a group of undergraduate students the space to explore the creative potential of the technology they use every day-–-to make, break, mend, hack, learn and unlearn.
 
With the current rate of development in technology, we can't sit around and wait for budget, interest and motivation to align perfectly, or the creation of new, multi-million dollar facilities. By the time they're built, we've moved on. We need to create more spaces for our students, and ourselves, to be creative and nimble, moving with and ahead of trends and innovations.
 
New Media departments (either inside the Design school or elsewhere) can't be just another silo. The more we speak to students from different parts of the university, the more we realise that the next generation of new media creators are spread across the whole campus. They are a complex group with a wide range of goals, interests, skills, identities and backgrounds. Our aim is to create a space in which these curious minds can explore freedom from the academic silos they inherited from previous generations and sculpt their own technological landscape.},
  eventtitle = {Code: {{A Media Conference}} of {{Platforms}}, {{Devices}} and {{Screens}}},
  booktitle = {Code: {{A Media Conference}} of {{Platforms}}, {{Devices}} and {{Screens}}},
  date = {2018-11-19},
  author = {Swift, Ben and Hunter, Alexander}
}

@article{swiftDesignSmartphonebasedDigital2012,
  langid = {australian},
  title = {The Design of a Smartphone-Based Digital Musical Instrument for Jamming},
  url = {https://openresearch-repository.anu.edu.au/handle/1885/151432},
  doi = {10.25911/5d5157b18b58f},
  abstract = {Open-ended human-computer interactions, such as those in interactive digital art and music, are an increasingly popular area of study in HCI. They provide an opportunity to examine playfulness, creativity and expression and challenge conventional HCI notions of quality, evaluation and how to measure success.  Jamming-improvisational group music making-is often held up as an example of open-ended creativity. This thesis describes the development of Viscotheque, an iPhone- based digital musical instrument (DMI) designed for jamming, over three major design-test cycles. Over these three iterations the interface evolved from a very simple 'process control'  interface in v1 to a more expressive multi-touch sample manipulation tool in v3. At each stage of the design process, open-ended jam sessions held with local musicians suggested that the potential was there for the interface to support rich jamming experiences. Version 3 of the interface and the associated v3 jam session was the most in-depth of the three phases of the experiment, with the most expressive interface and also the most comprehensive field trial (using a multi-session longitudinal study of jamming musicians rather than the single jam sessions of v1 and v2).  Situating the qualitative results of these experiments within the broader context of third wave HCI, this thesis discusses 'affect' in a guise perhaps unfamiliar to readers of mainstream HCI discourse. The jam sessions were characterised by intense sonic atmospheres, and the post-jam interviews reveal a complicated picture of agency in the interaction of the musician and their sound. The thesis also presents a detailed analysis of the quantitative log data, including the results of a Machine Learning (ML) approach to looking for patterns in this data.  Finally, the thesis discusses the implications of the Viscotheque design process for HCI more broadly, including the powerful affective atmospheres which characterise musical interaction and an approach to data analysis which leverages the mathematical sophistication of modern ML techniques while remaining sensitive to the difficulties surrounding the measurement of experience. -- provided by Candidate.},
  journaltitle = {ANU PhD Thesis},
  urldate = {2019-09-20},
  date = {2012},
  author = {Swift, Ben},
  file = {/Users/ben/Documents/Zotero/storage/JX4PQ6AW/swift_2012_the_design_of_a_smartphone-based_digital_musical_instrument_for_jamming.pdf}
}

@inproceedings{heReducingLatencyCollaborative2019,
  langid = {english},
  location = {{Brisbane, QLD, Australia}},
  title = {Reducing {{Latency}} in a {{Collaborative Augmented Reality Service}}},
  isbn = {978-1-4503-7002-8},
  url = {http://dl.acm.org/citation.cfm?doid=3359997.3365699},
  doi = {10.1145/3359997.3365699},
  eventtitle = {The 17th {{International Conference}} on {{Virtual}}-{{Reality Continuum}} and Its {{Applications}} in {{Industry}}},
  booktitle = {The 17th {{International Conference}} on {{Virtual}}-{{Reality Continuum}} and Its {{Applications}} in {{Industry}} on   - {{VRCAI}} '19},
  publisher = {{ACM Press}},
  urldate = {2020-01-09},
  date = {2019},
  pages = {1-9},
  author = {He, Wennan and Swift, Ben and Gardner, Henry and Xi, Mingze and Adcock, Matt},
  file = {/Users/ben/Documents/Zotero/storage/Z92GTSM2/he_et_al_2019_reducing_latency_in_a_collaborative_augmented_reality_service.pdf}
}

@inproceedings{swiftTwoPerspectivesRebooting2019,
  location = {{Melbourne, Australia}},
  title = {Two {{Perspectives}} on {{Rebooting Computer Music Education}}: {{Composition}} and {{Computer Science}}},
  isbn = {1448-7780},
  url = {http://computermusic.org.au/conferences/acmc-2019/},
  eventtitle = {{{ACMC}} '19},
  booktitle = {Proceedings of the 2019  {{Australasian Computer Music Conference}}},
  publisher = {{Australasian Computer Music Association}},
  date = {2019},
  author = {Swift, Ben and Martin, Charles Patrick and Hunter, Alexander},
  file = {/Users/ben/Documents/Zotero/storage/IRQPLTWL/swift_et_al_2019_two_perspectives_on_rebooting_computer_music_education.pdf}
}

@inproceedings{martinTrackingEnsemblePerformance2015,
  location = {{Baton Rouge, Louisiana, USA}},
  title = {Tracking Ensemble Performance on Touch-Screens with Gesture Classification and Transition Matrices},
  isbn = {978-0-692-49547-6},
  url = {https://dl.acm.org/doi/10.5555/2993778.2993870},
  doi = {10.5555/2993778.2993870},
  booktitle = {{{NIME}} 15: {{Proceedings}} of the International Conference on New Interfaces for Musical Expression},
  series = {{{NIME}} 2015},
  publisher = {{The School of Music and the Center for Computation and Technology (CCT), Louisiana State University}},
  date = {2015},
  pages = {359-364},
  author = {Martin, Charles and Gardner, Henry and Swift, Ben},
  file = {/Users/ben/Documents/Zotero/storage/4BP755NV/martin_et_al_2015_tracking_ensemble_performance_on_touch-screens_with_gesture_classification_and.pdf},
  numpages = {6}
}


